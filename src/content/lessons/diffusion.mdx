---
title: 'Denoising Diffusion Probabilistic Models (DDPM) and links with Flow Matching'
menu: 'Diffusion models (DDPM)'
description: 'On diffusion models and their core principles, and links with flow matching'
pubDate: '2025-10-08'
heroImage: '../../assets/blog-placeholder-3.jpg'
order: 510
tags: ['beta', 'Diffusion', 'Flow Matching']
typst: ['del', 'code']
---

import T from '../../components/TypstMath.astro'
import Cetz from '../../components/TypstCetz.astro'
import { Image } from 'astro:assets'
import diffusion from '../../assets/diffusion.svg'
import InlineSvg from '../../components/InlineSvg.astro'
import Counter from '../../components/numbering/Counter.astro'
import Ref from '../../components/numbering/Ref.astro'


<div class="glossary">
<label><input type="checkbox" checked /> Glossary (keep visible)</label>
- **~beta_t~** ((choice)) small added noise variance at step ~t~
- **~alpha_t := 1 - beta_t~** signal retention factor at step ~t~
- **~overline(alpha)_ t := product_(s=1)^t alpha_s~** overall signal retention factor from the start to step ~t~
- **~1 - overline(alpha)_t~** overall noise variance added from the start to step ~t~
- **~sigma_t^2~** ((choice)) variance of the backward step at step ~t~
- **~q(x_t | x_(t-1))~** ~!:= cal(N)(sqrt(1 - beta_t)x_(t-1), beta_t I)~ forward/noising step distribution
- **~q(x_t | x_0)~**      ~!= cal(N)(sqrt(overline(alpha)_t)  x_0, (1 - overline(alpha)_t)  I)~ big-jump forward/noising step distribution
- **~q(x_(t-1)|x_0)~**    ~!= cal(N)(sqrt(overline(alpha)_ (t-1))x_0, (1 - overline(alpha)_(t-1))I)~ big-jump for step ~t-1~
</div>

This lesson introduces the principles of diffusion models.
The aim is to clearly explain, with both the intuition and the detailed derivation, the original Denoising Diffusion Probabilistic Models (DDPM).
The derivations are quite detailed, including some parts that are usually skipped in papers/blogs.

Diffusion models predate flow matching models, and were the first models to achieve high-quality image generation results comparable to GANs.
The standard diffusion setting aims at learning to map a noise distribution, typically $N(0,I)$ via a continuous normalizing flow.
Even though the formulation is quite different and stochastic, in many ways, diffusion models can be seen as a special case of flow matching models, with a specific choice of noise distribution and trajectory parametrization.

A key breakthrough of diffusion models was to be able to specify the complete evolution of the probability distribution between noise and data.
This allows to supervise the generative/denoising process (a continuous normalizing flow) at every step instead of just specifying the distribution at the first and last steps.

To achieve this, diffusion models imagine a forward noising process, which progressively adds noise to the data until it becomes pure noise.
The generative process is then trained to reverse this noising process, and denoise the data step by step.







## Common knowledge about Gaussians / normal distributions

The goal here is not to define the normal distribution, nor to explain all its properties.
We will just recall some properties of Gaussians that will be useful in the rest of the lesson.

### KL divergence between two normals

The KL divergence between two normal distributions ~cal(N)(mu_1, sigma_1^2)~ and ~cal(N)(mu_2, sigma_2^2)~ is given by:

~!"KL"(cal(N)(mu_1, sigma_1^2) || cal(N)(mu_2, sigma_2^2)) = (mu_1 - mu_2)^2 / (2 sigma_2^2) + (sigma_1^2 / (2 sigma_2^2)) - 1/2 + log(sigma_2 / sigma_1)~

In the case we will use it, only the first term will be useful: it will be the only term depending on the variable of interest.

### Product of two normal densities

The product of two normal densities ~cal(N)(mu_1, sigma_1^2)~ and ~cal(N)(mu_2, sigma_2^2)~ is proportional to another normal density:

~!cal(N)(mu_1, sigma_1^2) times cal(N)(mu_2, sigma_2^2) prop cal(N)((mu_1 / sigma_1^2 + mu_2 / sigma_2^2) / (1 / sigma_1^2 + 1 / sigma_2^2), 1 / (1 / sigma_1^2 + 1 / sigma_2^2))~

or more explicitly,

~! exists K_1, mu, sigma, forall x: cal(N)(mu_1, sigma_1^2)(x) times cal(N)(mu_2, sigma_2^2)(x) = K_1 times cal(N)(mu, sigma)(x)~

with ~!mu = (mu_1 / sigma_1^2 + mu_2 / sigma_2^2) / (1 / sigma_1^2 + 1 / sigma_2^2) = (sigma_2^2 mu_1 + sigma_1^2 mu_2) / (sigma_1^2 + sigma_2^2)~
and ~!sigma^2 = 1 / (1 / sigma_1^2 + 1 / sigma_2^2) = (sigma_1^2  sigma_2^2) / (sigma_1^2 + sigma_2^2)~

<details>
<summary>Proof</summary>
<div>
We work on the log space, focusing/keeping only the terms depending on ~x~ (the rest is the normalization constant of a Gaussian).
More generally, the log of a gaussian density ~cal(N)(mu, sigma^2)~ can be expanded as:
<T block v='
ln(cal(N)(mu, sigma^2)(x)) & = -1/2 ((x - mu)^2 / sigma^2) + C_1 \
& = -1/2 (x^2 / sigma^2 - 2 (mu x) / sigma^2 + mu^2 / sigma^2) + C_1 \
& = -1/(2 sigma^2) (x^2 - 2 mu x) + C_2 \
'/>

This formula is useful for identifying the parameters of a Gaussian density from its expanded log density.

For the product of two Gaussian densities, we have:
<T block v='
& ln(cal(N)(mu_1, sigma_1^2)(x) times cal(N(mu_2, sigma_2^2)(x))) \
& = - 1/2 ((x - mu_1)^2 / sigma_1^2 + (x - mu_2)^2 / sigma_2^2) + C_3 \
& "(developing)" \
& = -1/2 (x^2 / sigma_1^2 - 2 (mu_1 x) / sigma_1^2 + mu_1^2 / sigma_1^2 + x^2 / sigma_2^2 - 2 (mu_2 x) / sigma_2^2 + mu_2^2 / sigma_2^2) + C_3 \
& "(factorizing and pushing constant terms in the constant)" \
& = -1/2 ((1 / sigma_1^2 + 1 / sigma_2^2) x^2 - 2 (mu_1 / sigma_1^2 + mu_2 / sigma_2^2) x) + C_4 \
& = -1/2 (x^2 / sigma^2 - 2 (mu / sigma^2) x) + C_4 \
'/>
We can identify ~sigma^2~ directly, and then deduce ~mu~:
<T block v='
sigma^2 & = 1 / (1 / sigma_1^2 + 1 / sigma_2^2) = (sigma_1^2 sigma_2^2) / (sigma_1^2 + sigma_2^2) \
mu & = sigma^2 (mu_1 / sigma_1^2 + mu_2 / sigma_2^2) = (sigma_2^2 mu_1 + sigma_1^2 mu_2) / (sigma_1^2 + sigma_2^2) \
'/>
</div>
</details>

### Identities on normal mean

- ~cal(N)(mu, sigma^2)(x) = cal(N)(x, sigma^2)(mu)~
- ~cal(N)(a mu, sigma^2)(x) = cal(N)(mu, sigma^2 / a^2)(x / a)~
- combining both: ~cal(N)(a x, sigma^2)(mu) = cal(N)(mu / a, sigma^2 / a^2)(x)~






## Forward/noising process

In this section, we introduce the forward/noising process, and derive all properties that are necessary afterwards.


### Global view

The forward/noising process progressively adds noise to the data until it becomes pure noise.
As shown in Fig. <Ref label="fig:global-noise"/>, the forward process starts from data points
~forall i, x^i_0 tilde.op q(x_0)~ (e.g. images from the training set, also named ~hat(p)_"data"~).
The forward process progressively adds Gaussian noise to these data points, until they are completely shuffled and become close to pure noise.
The total process is run for a finite (but high) number of steps ~T~, and we obtain ~forall i, x^i_T tilde.op cal(N)(0,I)~.

<figure>
    <InlineSvg asset="diffusion" hide='#FORWARD, #BACKWARD, #forward, #backward , #more, #bigkl' />
    <figcaption>[<Counter label="fig:global-noise"/>] global forward noising process.</figcaption>
</figure>

### Local view

The forward/noising process is defined as a Markov chain, where each step adds a bit of Gaussian noise to the previous step.
As shown in Fig. <Ref label="fig:one-step-noise"/> the position at step ~t~ depends only on the position at step ~t-1~ (hence the Markov property).
More precisely, the forward/noising process is defined as:

<T block v="
x_0 & tilde.op q(x_0) \
forall t in [1..T], x_t & tilde.op q(x_t | x_(t-1)) \
"/>

Even more precisely:
- we can decide on a variance addition schedule ~beta_1, ..., beta_T~ saying how much noise variance to add at each step,
- as we add noise at each step, the distribution would be more and more spread along time steps (~t~) and would not reach a gaussian noise with identity covariance,
- to avoid this, we also rescale the signal at each step by a factor ~sqrt(1 - beta_t)~.

The goal of the rescaling is to ensure that the variance at step ~t~ is always ~1~, whatever ~t~.
Overall the forward/noising process is defined as:

<T block v="
x_0 & tilde.op q(x_0) \
forall t in [1..T], x_t & tilde.op q(x_t | x_(t-1)) = cal(N)(sqrt(1 - beta_t) x_(t-1), beta_t I) \
"/>

<figure>
    <InlineSvg asset="diffusion" hide='#BACKWARD, #qt, #qtt0, #forward, #backward , #more, #bigkl' />
    <figcaption>[<Counter label="fig:one-step-noise"/>] One-step noising process.</figcaption>
</figure>


### Big-jump view

Thanks to the properties of Gaussians, we can express the distribution at step ~t~ as a function of the initial data point ~x_0~.
Indeed, since each step is a Gaussian distribution, the composition of all the steps is also a Gaussian distribution as shown in Fig. <Ref label="fig:multi-step-noise"/>.
More precisely, we have:

<T block v="
forall t in [1..T], x_t & tilde.op q(x_t | x_0) = cal(N)(sqrt(overline(alpha)_t) x_0, (1 - overline(alpha)_t) I) \
"/>

<figure>
    <InlineSvg asset="diffusion" hide='#BACKWARD, #qtt0, #backward, #more, #bigkl' />
    <figcaption>[<Counter label="fig:multi-step-noise"/>] Multi-step noising process.</figcaption>
</figure>



### Sandwich view

As this will become useful, we can also express the distribution at step ~t-1~ as a function of the position at step ~t~ and the initial data point ~x_0~.
This is illustrated in Fig. <Ref label="fig:sandwich-noise"/>:
- the two densities in blue correspond to the information on ~x_(t-1)~ that we have thanks to ~x_t~ (the peaky one) and ~x_0~ (the wide one),
- their product (in pink), once renormalized, gives us the distribution of ~x_(t-1)~ knowing both ~x_t~ and ~x_0~

More precisely, we have:

<T block v="
forall t in [1..T], q(x_(t-1) | x_t, x_0) & prop q(x_(t-1) | x_t) times q(x_(t-1) | x_0) \
"/>

We will show that we can derive a closed-form expression for this distribution:

<T block v='
forall t in [1..T], q(x_(t-1) | x_t, x_0) & prop cal(N)(tilde(mu), tilde(beta) I) \
"with" \
tilde(mu)(x_0, t, x_t) & = (sqrt(overline(alpha)_(t-1)) beta_t)/ (1 - overline(alpha)_t) x_0 + (sqrt(alpha_t) (1- overline(alpha)_(t-1))) / (1 - overline(alpha)_t) x_t \
tilde(beta)(t) & = (1 - overline(alpha)_(t-1)) / (1 - overline(alpha)_t) beta_t \
'/>

The proof relies on showing that ~q(x_(t-1) | x_t)~ (the anti-forward step) is proportional to a Gaussian density, and then using the product-of-two-Gaussians property that we derived in the preliminaries.

<details>
<summary>Derivation</summary>
<div>

Knowing ~x_0~, we can express the anti-forward step ~q(x_(t-1) | x_t, x_0)~ using the Bayes rule.
Remembering that this is a distribution over ~x_(t-1)~, we can focus on the factors that depend on it (and thus drop $q(x_t)$ below):

<T block v='
forall t in [1..T], q(x_(t-1) | x_t, x_0) & = q(x_t | x_(t-1), x_0) q(x_(t-1) | x_0) / q(x_t | x_0) \
& prop q(x_t | x_(t-1)) q(x_(t-1) | x_0) \
& prop cal(N)(sqrt(1 - beta_t) x_(t-1), beta_t I)(x_t) times q(x_(t-1) | x_0) \
cal(N)(a x,sigma^2)(mu) => cal(N)(μ/a, σ^2/a^2)(x) "   "
& prop cal(N)(x_t / sqrt(1 - beta_t), beta_t / (1 - beta_t) I)(x_(t-1)) \
& "   " times cal(N)(sqrt(overline(alpha)_(t-1)) x_0, (1 - overline(alpha)_(t-1)) I)(x_(t-1)) \
"(prod. of gaussian densities)   "
& prop cal(N)(tilde(mu), tilde(beta))(x_(t-1)) \

"with" \
tilde(beta)
& = (( beta_t / (1 - beta_t) ) ( 1 - overline(alpha)_(t-1))) / (( beta_t / (1 - beta_t) ) + ( 1 - overline(alpha)_(t-1))) \
& = (1 - overline(alpha)_(t-1)) / (beta_t + (1-beta_t)(1 - overline(alpha)_t)) beta_t \
& = (1 - overline(alpha)_(t-1)) / (1 - alpha_t + alpha_t (1 - overline(alpha)_t)) beta_t \
& = (1 - overline(alpha)_(t-1)) / (1 - overline(alpha)_t) beta_t \

"and" \
tilde(mu) & = ((x_t / sqrt(1 - beta_t)) (1 - overline(alpha)_(t-1)) + (sqrt(overline(alpha)_(t-1)) x_0) (beta_t / (1 - beta_t))) / (( beta_t / (1 - beta_t) ) + ( 1 - overline(alpha)_(t-1))) \
"(as for "mu", pass "1 - beta_t" down)   "
& = (sqrt(alpha_(t-1)) beta_t) / (1 - overline(alpha)_t) x_0
  + (sqrt(alpha_t) (1 - overline(alpha)_(t-1))) / (1 - overline(alpha)_t) x_t \
'/>


</div>
</details>



<figure>
    <InlineSvg asset="diffusion" hide='#BACKWARD, #backward, #more, #bigkl' lighten="#qt, #qtt" />
    <figcaption>[<Counter label="fig:sandwich-noise"/>] Closed-form conditional backward step.</figcaption>
</figure>

## Learnable backward process

Inspired by the forward/noising process, we define a backward/denoising process that will have parameters so that we can learn it to reverse the forward/noising process.
As shown in Fig. <Ref label="fig:learnable-backward"/>, the backward/denoising process starts from some random noise.
It is also defined as a Markov chain, where each step ~t-1~ only depends on the previous step ~t~.

Said differently, the process is defined by the distributions ~p_theta (x_(t-1) | x_t)~ for ~t in [1..T]~, where ~theta~ are the learnable parameters of the model.
The form of ~p_theta~ is actually a gaussian distribution, with a mean predicted by a neural network, and a variance that can be fixed or learned.
We typically have, with a fixed denoising variance schedule ~sigma_1, ..., sigma_T~, and finally write:

<T block v='
x_T & tilde.op p(x_T) = cal(N)(0, I) \
forall t in [1..T], x_(t-1) & tilde.op p_theta(x_(t-1) | x_t) = cal(N)(mu_theta (x_t, t), sigma_t^2 I) \
'/>


<figure>
    <InlineSvg asset="diffusion" hide='#FORWARD, #forward, #kl, #more, #bigkl' />
    <figcaption>[<Counter label="fig:learnable-backward"/>] Learnable backward process.</figcaption>
</figure>


### Overall goal: how to guide learning?

The parameters ~theta~ of the backward/denoising process will be learned by fitting the backward steps to the forward steps.
More precisely, we want to minimize the KL divergence between the distribution induced by the forward step ~q(x_(0:T))~ and one induced by the backward step  ~p_theta (x_(0:T))~, as hinted in Fig. <Ref label="fig:fitting-process"/>.

<figure>
    <InlineSvg asset="diffusion" hide='#BACKWARD, #FORWARD'/>
    <figcaption>[<Counter label="fig:fitting-process"/>] Fitting the forward process with the backward process.</figcaption>
</figure>


## From global to local KL

This part details the key insight of how to transform a global KL loss into a sum of local ones, eventually expressed as square errors.

### Overview

We will show that we can simplify the global objective into a sum of local objectives, one for each step, with the proper conditioning.
We will follow a few steps:
- starting by writing the KL divergence between the two joint distributions of the Markov chains, both backward in time,
- decomposing the KL divergence as a sum of KL divergences between each step, being careful on the writting, expectations and conditionings,
- "reversing" the forward noising process and making it tractable by conditioning on some ~x_0~ (and using the "sandwich view").

The final terms involved in the loss are KL divergences between two Gaussian distributions, which can be computed in closed form.
These are illustrated in Fig. <Ref label="fig:temporal-locality"/>.

### Goal: KL divergence between the forward and backward processes

We decompose the global KL divergence between the forward noising process and the backward learnable process.

<T block v='
cal(L)
& := "KL"(q(x_(0:T)) || p_theta(x_(0:T))) \
& = EE_(x_(0:T) tilde.op q(x_(0:T))) [ln (q(x_(0:T)) / (p_theta (x_(0:T))))] \
& = EE_(x_(0:T) tilde.op q(x_(0:T))) [ln (q(x_T) times product_(t=1)^T q(x_(t-1) | x_t)) - ln (p_theta (x_T) times product_(t=1)^T p_theta (x_(t-1) | x_t))] \
& = EE_(x_(0:T) tilde.op q(x_(0:T))) [ln (q(x_T) / (p_theta (x_T)) + sum_(t=1)^T ln ( q(x_(t-1) | x_t)) / (p_theta (x_(t-1) | x_t)))] \
& = EE_(x_T tilde.op q(x_T)) [ln (q(x_T) / (p_theta (x_T)))] + sum_(t=1)^T EE_(x_t tilde.op q(x_t)) EE_(x_(t-1) tilde.op q(x_(t-1) | x_t)) [ln ( q(x_(t-1) | x_t)) / (p_theta (x_(t-1) | x_t))] \
//& = "KL"(q(x_T) || p_theta (x_T)) + sum_(t=1)^T EE_(x_t tilde.op q(x_t)) ["KL"(q(x_(t-1) | x_t) || p_theta (x_(t-1) | x_t))] \
'/>

### Making it tractable with sandwiches

We will use ~p(A) = sum_B p(A | B) p(B) = EE_B [p(A|B)]~ to introduce ~x_0~ and get the sandwich view (that we pre-solved earlier).

<T block v='
q(x_(t-1) | x_t)
& = EE_(x_0 tilde.op q(x_0)) [q(x_(t-1) | x_t, x_0)] \
'/>

<details>
<summary>The case of ~t=1~</summary>
<div>
The case of ~t=1~ is a bit special, as we ~x_(t-1) eq x_0~ and so no "sandwich".
The initial term is directly ~q(x_0 | x_1)~, i.e., ~q(x_0 | x_1) = cal(N)(~.
</div>
</details>

This formula with an expectation on ~q(x_0)~ (i.e. ~p_"data"~) hints for a closed form solution with a sum, in case of a finite dataset.
Technically, this means a mixture of gaussian as each ~q(x_(t-1) | x_t, x_0^i)~ is a Gaussian.
This could be explored further but is not very practical.

To get the usual diffusion loss, it seems we have to resort to the Jensen's inequality to get an upper bound.
This is a bit unsatisfactory as we loose the exactness of the KL minimization.
This step is not explicit in other presentations of diffusion models, but seems necessary to get the usual diffusion loss (this might need further attention).

<T block v='
ln (q(x_(t-1) | x_t) / p_theta (x_(t-1) | x_t))
& = ln ((EE_(x_0 tilde.op q(x_0)) [q(x_(t-1) | x_t, x_0)]) / (p_theta (x_(t-1) | x_t))) \
 "(Jensen + denominator indep. of " x_0 ")" \
& <= EE_(x_0 tilde.op q(x_0)) [ln (q(x_(t-1) | x_t, x_0) / (p_theta (x_(t-1) | x_t)))] \
'/>

In the end we can identify the KL divergences and get the final upper bound:

<T block v='
cal(L)
& <= "KL"(q(x_T) || p_theta (x_T)) \
& + "KL"(q(x_0|x_1) || p_theta (x_0|x_1)) \
& + sum_(t=2)^T EE_(x_t tilde.op q(x_t)) EE_(x_0 tilde.op q(x_0)) ["KL"(q(x_(t-1) | x_t, x_0) || p_theta (x_(t-1) | x_t))] \
'/>

All involved distributions are either fixed (at ~t=T~) or Gaussian with known closed-form (the others).
We thus get a closed-form expression of the upper bound of the loss, involving square distances.

<T block v='
cal(L)
& <= "KL"(cal(N)(0,I) || cal(N)(0,I)) \
& + "KL"(cal(N)(tilde(mu)(x_0, 1, x_1), tilde(beta)(1)^2 I) || cal(N)(mu_theta (x_1, 1), sigma_1^2 I)) \
& + sum_(t=2)^T EE_(x_t tilde.op q(x_t)) EE_(x_0 tilde.op q(x_0)) ["KL"(cal(N)(tilde(mu)(x_0, t, x_t), tilde(beta)(t)^2 I) || cal(N)(mu_theta (x_t, t), sigma_t^2 I))] \
& = 0 \
& + 1/(2 sigma_1^2) EE_(x_1 tilde.op q(x_1)) EE_(x_0 tilde.op q(x_0)) [(tilde(mu)(x_0, 1, x_1) - mu_theta (x_1, 1))^2] + C_1 \
& + sum_(t=2)^T 1/(2 sigma_t^2) EE_(x_t tilde.op q(x_t)) EE_(x_0 tilde.op q(x_0)) [(tilde(mu)(x_0, t, x_t) - mu_theta (x_t, t))^2] + C_t \
'/>


<figure>
    <InlineSvg asset="diffusion" hide='#forward, #backward, #more, #bigkl, #qt, #qtt' />
    <figcaption>[<Counter label="fig:temporal-locality"/>] Temporal locality of the learning process.</figcaption>
</figure>



<details>
<summary>All-in-one visual</summary>
<div>
<figure>
    <InlineSvg asset="diffusion" hide='#more, #backward' />

    <figcaption>[<Counter label="fig:all-together"/>] All together.</figcaption>
</figure>
</div>
</details>



## Aside: Bayes rule over a Markov chain

Even though the forward/noising process is defined forward in time, we can also express it in the opposite direction.
It corresponds to a different decomposition of the joint distribution of the Markov chain $q(x_0, ..., x_T)$:

<T block v='
& q(x_(0:T))
& = q(x_0) times product_(t=1)^T q(x_t | x_(t-1)) \
"(anti-forward)" 
&& = q(x_T) times product_(t=1)^T q(x_(t-1) | x_t) \
'/>

<details>
<summary>A different view / proof</summary>
<div>
We can develop the markov chain, and apply the Bayes rule at each step:

<T block v='
ln q(x_(0:T))
& := ln ( q(x_0) times product_(t=1)^T q(x_t | x_(t-1)) ) \
& = ln q(x_0) + sum_(t=1)^T ln q(x_t | x_(t-1)) \
& = ln q(x_0) + sum_(t=1)^T ln ( (q(x_(t-1) | x_t) q(x_t)) / q(x_(t-1)) ) \
& = ln q(x_0) + sum_(t=1)^T ln q(x_(t-1) | x_t) + sum_(t=1)^T ln q(x_t) - sum_(t=1)^T ln q(x_(t-1)) \
& = ln q(x_0) + sum_(t=1)^T ln q(x_(t-1) | x_t) + sum_(t=1)^T ln q(x_t) - sum_(t=0)^(T-1) ln q(x_t) \
& = ln q(x_0) + sum_(t=1)^T ln q(x_(t-1) | x_t) + ln q(x_T) - ln q(x_0) \
& = sum_(t=1)^T ln q(x_(t-1) | x_t) + ln q(x_T) \
'/>
</div>
</details>


