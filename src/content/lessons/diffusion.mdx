---
title: 'Denoising Diffusion Probabilistic Models (DDPM) and links with Flow Matching'
description: 'On diffusion models and their core principles, and links with flow matching'
pubDate: '2025-10-08'
heroImage: '../../assets/blog-placeholder-3.jpg'
order: 510
tags: ['draft', 'Diffusion', 'Flow Matching']
typst: 'del'
---

import T from '../../components/TypstMath.astro'
import Cetz from '../../components/TypstCetz.astro'
import { Image } from 'astro:assets'
import diffusion from '../../assets/diffusion.svg'
import InlineSvg from '../../components/InlineSvg.astro'
import Counter from '../../components/numbering/Counter.astro'
import Ref from '../../components/numbering/Ref.astro'

{ /*
Test
- dollar $(a+b)/2$
- code `(a+b)/2`
- del ~(a+b)/2~
- strong **(a+b)/2**
- T <T>(a+b) / 2</T>
*/ }

This lesson introduces the principles of diffusion models.
The aim is to clearly explain, with both the intuition and the detailed derivation, the original Denoising Diffusion Probabilistic Models (DDPM).

Diffusion models predate flow matching models, and were the first models to achieve high-quality image generation results comparable to GANs.
The standard diffusion setting aims at learning to map a noise distribution, typically $N(0,I)$ via a continuous normalizing flow.
Even though the formulation is quite different and stochastic, in many ways, diffusion models can be seen as a special case of flow matching models, with a specific choice of noise distribution and trajectory parametrization.

A key breakthrough of diffusion models was to be able to specify the complete evolution of the probability distribution between noise and data.
This allows to supervise the generative/denoising process (a continuous normalizing flow) at every step instead of just specifying the distribution at the first and last steps.

To achieve this, diffusion models imagine a forward noising process, which progressively adds noise to the data until it becomes pure noise.
The generative process is then trained to reverse this noising process, and denoise the data step by step.

## Common knowledge about Gaussians / normal distributions

The goal here is not to define the normal distribution, nor to explain all its properties.
We will just recall some properties of Gaussians that will be useful in the rest of the lesson.

### KL divergence between two normals

The KL divergence between two normal distributions ~cal(N)(mu_1, sigma_1^2)~ and ~cal(N)(mu_2, sigma_2^2)~ is given by:

~!"KL"(cal(N)(mu_1, sigma_1^2) || cal(N)(mu_2, sigma_2^2)) = (mu_1 - mu_2)^2 / (2 sigma_2^2) + (sigma_1^2 / (2 sigma_2^2)) - 1/2 + log(sigma_2 / sigma_1)~

### Product of two normal densities

The product of two normal densities ~cal(N)(mu_1, sigma_1^2)~ and ~cal(N)(mu_2, sigma_2^2)~ is proportional to another normal density:

~!cal(N)(mu_1, sigma_1^2) times cal(N)(mu_2, sigma_2^2) prop cal(N)((mu_1 / sigma_1^2 + mu_2 / sigma_2^2) / (1 / sigma_1^2 + 1 / sigma_2^2), 1 / (1 / sigma_1^2 + 1 / sigma_2^2))~

or more explicitly,

~! exists K_1, forall x: cal(N)(mu_1, sigma_1^2)(x) times cal(N)(mu_2, sigma_2^2)(x) = K_1 times cal(N)(tilde(mu), tilde(sigma))(x)~

with ~!tilde(mu) = (mu_1 / sigma_1^2 + mu_2 / sigma_2^2) / (1 / sigma_1^2 + 1 / sigma_2^2) = (sigma_2^2 mu_1 + sigma_1^2 mu_2) / (sigma_1^2 + sigma_2^2)~
and ~!tilde(sigma)^2 = 1 / (1 / sigma_1^2 + 1 / sigma_2^2) = (sigma_1^2  sigma_2^2) / (sigma_1^2 + sigma_2^2)~

### Identities on normal mean

- ~cal(N)(mu, sigma^2)(x) = cal(N)(x, sigma^2)(mu)~
- ~cal(N)(a mu, sigma^2)(x) = cal(N)(mu, sigma^2 / a^2)(x / a)~
- combining both: ~cal(N)(a x, sigma^2)(mu) = cal(N)(mu / a, sigma^2 / a^2)(x)~

## Description and properties of the forward/noising process

<figure>
    <InlineSvg asset="diffusion" hide='#FORWARD, #BACKWARD, #forward, #backward , #more, #bigkl' />
    <figcaption>[<Counter label="fig:global-noise"/>] global forward noising process.</figcaption>
</figure>

See Fig. <Ref label="fig:one-step-noise"/> for an illustration of the forward/noising process.

<figure>
    <InlineSvg asset="diffusion" hide='#BACKWARD, #qt, #qtt0, #forward, #backward , #more, #bigkl' />
    <figcaption>[<Counter label="fig:one-step-noise"/>] One-step noising process.</figcaption>
</figure>

See Fig. <Ref label="fig:global-noise"/> for an illustration of the forward/noising process.

<figure>
    <InlineSvg asset="diffusion" hide='#BACKWARD, #qtt0, #backward, #more, #bigkl' />
    <figcaption>[<Counter label="fig:multi-step-noise"/>] Multi-step noising process.</figcaption>
</figure>

See Fig. <Ref label="fig:multi-step-noise"/> for an illustration of the forward/noising process.

<figure>
    <InlineSvg asset="diffusion" hide='#BACKWARD, #backward, #more, #bigkl' lighten="#qt, #qtt" />
    <figcaption>[D] Closed-form conditional backward step.</figcaption>
</figure>

<figure>
    <InlineSvg asset="diffusion" hide='#FORWARD, #forward, #kl, #more, #bigkl' />
    <figcaption>[E] Learnable backward process.</figcaption>
</figure>

<figure>
    <InlineSvg asset="diffusion" hide='#BACKWARD, #FORWARD'/>
    <figcaption>[F] Fitting the forward process with the backward process.</figcaption>
</figure>

<figure>
    <InlineSvg asset="diffusion" hide='#forward, #backward, #more, #bigkl, #qt, #qtt' />
    <figcaption>[G] Temporal locality of the learning process.</figcaption>
</figure>



<figure>
    <InlineSvg asset="diffusion" hide='#more, #backward' />
    <figcaption>[Z] All together.</figcaption>
</figure>

